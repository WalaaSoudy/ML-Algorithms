{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1afb7b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy-based Decision Tree:\n",
      "Accuracy: 0.2\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "Confusion Matrix:\n",
      "[[1 0]\n",
      " [4 0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "# Assuming you have defined X_train_np, y_train_np, X_test_np, y_test earlier in your code\n",
    "# Read the data\n",
    "nflx_data = pd.read_csv(\"Download Data - STOCK_US_XNAS_NFLX.csv\")\n",
    "\n",
    "# Preprocess the data\n",
    "nflx_data['Volume'] = nflx_data['Volume'].str.replace(',', '').astype(float)\n",
    "median_volume = nflx_data['Volume'].median()\n",
    "nflx_data['High_Volume'] = (nflx_data['Volume'] > median_volume).astype(int)\n",
    "\n",
    "# Feature Selection\n",
    "X = nflx_data[['Open', 'Close', 'High', 'Low']]\n",
    "y = nflx_data['High_Volume']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Example usage\n",
    "dt_entropy_classifier_scratch = DecisionTreeEntropy(max_depth=3)\n",
    "dt_entropy_classifier_scratch.fit(X_train.values, y_train.values)\n",
    "\n",
    "# Predicting on the test set\n",
    "y_pred_dt_entropy_scratch = dt_entropy_classifier_scratch.predict(X_test.values)\n",
    "class Node:\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value\n",
    "\n",
    "class DecisionTreeEntropy:\n",
    "    def __init__(self, max_depth=None):\n",
    "        self.max_depth = max_depth\n",
    "        self.tree = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.tree = self._grow_tree(X, y, depth=0)\n",
    "\n",
    "    def _grow_tree(self, X, y, depth):\n",
    "        num_samples, num_features = X.shape\n",
    "        unique_classes = np.unique(y)\n",
    "\n",
    "        # Stop conditions\n",
    "        if len(unique_classes) == 1:\n",
    "            return Node(value=unique_classes[0])\n",
    "\n",
    "        if depth == self.max_depth:\n",
    "            return Node(value=self._majority_vote(y))\n",
    "\n",
    "        # Find the best split\n",
    "        best_feature, best_threshold = self._find_best_split(X, y)\n",
    "\n",
    "        # Split the data\n",
    "        left_mask = X[:, best_feature] <= best_threshold\n",
    "        right_mask = ~left_mask\n",
    "\n",
    "        left_subtree = self._grow_tree(X[left_mask], y[left_mask], depth + 1)\n",
    "        right_subtree = self._grow_tree(X[right_mask], y[right_mask], depth + 1)\n",
    "\n",
    "        # Create the current node\n",
    "        return Node(feature=best_feature, threshold=best_threshold, left=left_subtree, right=right_subtree)\n",
    "\n",
    "    def _find_best_split(self, X, y):\n",
    "        num_samples, num_features = X.shape\n",
    "        classes = np.unique(y)\n",
    "        best_info_gain = -float('inf')\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "\n",
    "        for feature in range(num_features):\n",
    "            thresholds = np.unique(X[:, feature])\n",
    "            for threshold in thresholds:\n",
    "                left_mask = X[:, feature] <= threshold\n",
    "                right_mask = ~left_mask\n",
    "\n",
    "                info_gain = self._information_gain(y, y[left_mask], y[right_mask])\n",
    "\n",
    "                if info_gain > best_info_gain:\n",
    "                    best_info_gain = info_gain\n",
    "                    best_feature = feature\n",
    "                    best_threshold = threshold\n",
    "\n",
    "        return best_feature, best_threshold\n",
    "\n",
    "    def _information_gain(self, parent, left_child, right_child):\n",
    "        entropy_parent = self._entropy(parent)\n",
    "        entropy_left = len(left_child) / len(parent) * self._entropy(left_child)\n",
    "        entropy_right = len(right_child) / len(parent) * self._entropy(right_child)\n",
    "\n",
    "        return entropy_parent - entropy_left - entropy_right\n",
    "\n",
    "    def _entropy(self, y):\n",
    "        classes, counts = np.unique(y, return_counts=True)\n",
    "        probabilities = counts / len(y)\n",
    "        entropy = -np.sum(probabilities * np.log2(probabilities + 1e-10))  # Adding a small value to prevent log(0)\n",
    "        return entropy\n",
    "\n",
    "    def _majority_vote(self, y):\n",
    "        classes, counts = np.unique(y, return_counts=True)\n",
    "        majority_class = classes[np.argmax(counts)]\n",
    "        return majority_class\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._predict_tree(x, self.tree) for x in X])\n",
    "\n",
    "    def _predict_tree(self, x, node):\n",
    "        if node.value is not None:\n",
    "            return node.value\n",
    "\n",
    "        if x[node.feature] <= node.threshold:\n",
    "            return self._predict_tree(x, node.left)\n",
    "        else:\n",
    "            return self._predict_tree(x, node.right)\n",
    "\n",
    "# Example usage\n",
    "\n",
    "dt_entropy_classifier_scratch = DecisionTreeEntropy(max_depth=3)\n",
    "dt_entropy_classifier_scratch.fit(X_train.values, y_train.values)\n",
    "\n",
    "# Predicting on the test set\n",
    "y_pred_dt_entropy_scratch = dt_entropy_classifier_scratch.predict(X_test.values)\n",
    "\n",
    "# Model Evaluation\n",
    "accuracy_dt_entropy_scratch = accuracy_score(y_test, y_pred_dt_entropy_scratch)\n",
    "precision_dt_entropy_scratch = precision_score(y_test, y_pred_dt_entropy_scratch, zero_division=0)\n",
    "recall_dt_entropy_scratch = recall_score(y_test, y_pred_dt_entropy_scratch)\n",
    "conf_matrix_dt_entropy_scratch = confusion_matrix(y_test, y_pred_dt_entropy_scratch)\n",
    "\n",
    "print(\"Entropy-based Decision Tree:\")\n",
    "print(\"Accuracy:\", accuracy_dt_entropy_scratch)\n",
    "print(\"Precision:\", precision_dt_entropy_scratch)\n",
    "print(\"Recall:\", recall_dt_entropy_scratch)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix_dt_entropy_scratch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620372d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
